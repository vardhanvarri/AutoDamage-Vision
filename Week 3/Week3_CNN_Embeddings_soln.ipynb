{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggoxfq3UyXrw"
      },
      "source": [
        "# Week 3 – CNN Embeddings on Vehicle Damage Images\n",
        "\n",
        "## Objective\n",
        "Learn how Convolutional Neural Networks automatically learn\n",
        "useful representations from vehicle damage images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M1ZYyhqjyZs-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vardh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(np, \"object\"):\n"
          ]
        }
      ],
      "source": [
        "# TODO: Import required libraries\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6XVtBzVyqVh"
      },
      "source": [
        "## Step 1: Load and Preprocess Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1tU-v7U0yrnk"
      },
      "outputs": [],
      "source": [
        "# TODO: Load vehicle damage images\n",
        "# Resize images (e.g., 128x128)\n",
        "# Normalize pixel values\n",
        "import json\n",
        "\n",
        "with open(\"annotations.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Build class mapping\n",
        "all_classes = set()\n",
        "for info in data.values():\n",
        "    for r in info[\"regions\"]:\n",
        "        all_classes.add(r[\"class\"])\n",
        "\n",
        "class_names = sorted(all_classes)\n",
        "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for img_name, info in data.items():\n",
        "    img_path = os.path.join(\"archive/image/image/\", img_name)\n",
        "\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    img = cv2.resize(img, (128, 128))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = img / 255.0  \n",
        "\n",
        "    label = info[\"regions\"][0][\"class\"]  \n",
        "\n",
        "    X.append(img)\n",
        "    y.append(class_to_idx[label])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoefeSe_ytev"
      },
      "source": [
        "## Step 2: Define CNN Architecture\n",
        "Build a small CNN to learn image features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "m8W221BazAlQ"
      },
      "outputs": [],
      "source": [
        "# TODO: Define a simple CNN\n",
        "# Conv -> ReLU -> Pool -> Conv -> Pool -> Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation=\"relu\", input_shape=(128, 128, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "    MaxPooling2D((2, 2)),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation=\"relu\"),\n",
        "    Dense(len(all_classes), activation=\"softmax\")\n",
        "])\n",
        "import numpy as np\n",
        "\n",
        "X = np.array(X, dtype=np.float32)\n",
        "y = np.array(y, dtype=np.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7eJs1JZzByS"
      },
      "source": [
        "## Step 3: Train the CNN\n",
        "This step will take time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wQeh8KRczK0K"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X shape: (11621, 128, 128, 3)\n",
            "y shape: (11621,)\n",
            "First y: 1.0\n",
            "Epoch 1/5\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 245ms/step - accuracy: 0.3086 - loss: 1.7980\n",
            "Epoch 2/5\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 215ms/step - accuracy: 0.4312 - loss: 1.5116\n",
            "Epoch 3/5\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 172ms/step - accuracy: 0.5733 - loss: 1.1643\n",
            "Epoch 4/5\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 168ms/step - accuracy: 0.7915 - loss: 0.6068\n",
            "Epoch 5/5\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 166ms/step - accuracy: 0.9419 - loss: 0.2025\n"
          ]
        }
      ],
      "source": [
        "# TODO: Compile and train the CNN model\n",
        "# Keep training short (few epochs)\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "print(\"First y:\", y[0])\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "history = model.fit(\n",
        "    X,\n",
        "    y,\n",
        "    epochs=5,\n",
        "    batch_size=32,\n",
        "    shuffle=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR5s8nl1zNro"
      },
      "source": [
        "## Step 4: Extract CNN Embeddings\n",
        "Extract features from the penultimate layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "a6ejwUAHzQFu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 41ms/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "\n",
        "embedding_model = Model(\n",
        "    inputs=model.inputs,\n",
        "    outputs=model.layers[-2].output\n",
        ")\n",
        "\n",
        "embeddings = embedding_model.predict(X)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kur2TDgtzSYR"
      },
      "source": [
        "## Step 5: PCA / t-SNE on CNN Embeddings\n",
        "Visualize learned representations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLdUuEM9zULW"
      },
      "outputs": [],
      "source": [
        "# TODO: Apply PCA or t-SNE to CNN embeddings\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Reduce first for stability\n",
        "pca_50 = PCA(n_components=50, random_state=42)\n",
        "emb_50 = pca_50.fit_transform(embeddings)\n",
        "\n",
        "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
        "emb_tsne = tsne.fit_transform(emb_50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHkBUo3WzWFG"
      },
      "source": [
        "## Reflection\n",
        "- How do CNN embeddings differ from HOG features?\n",
        "- Why do CNN features work better for damage detection?\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
